{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "  def __init__(self):\n",
    "    self.batch_size = 1\n",
    "    self.gpu_id = 0\n",
    "    self.train = False\n",
    "    self.checkpoint_dir = \"outputs\"\n",
    "    self.text_prompt = ''\n",
    "    self.output_dir = \"outputs\"\n",
    "    self.dataset_name = \"sample\"\n",
    "    self.train_data_dir = None\n",
    "    self.val_data_dir = None\n",
    "    self.test_data_dir = None\n",
    "    self.optimizer = \"Adam\"\n",
    "    self.epochs = 200\n",
    "    self.lr = 0.1\n",
    "    self.patch_threshold = 100\n",
    "    self.test_mask_size = 512\n",
    "    self.save_test_predictions = True\n",
    "    self.dice_coef = 10\n",
    "    self.boundary_coef = 0.1\n",
    "    self.focal_coef = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(prediction, mask):\n",
    "    intersection = prediction * mask\n",
    "    union = prediction + mask - intersection\n",
    "    return intersection.sum() / (union.sum() + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/ISDADS/aaa77907/miniconda3/envs/clipseg/lib/python3.10/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Dataset(TorchDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        train=True,\n",
    "        mask_size=352,\n",
    "        num_parts=1,\n",
    "        min_crop_ratio=0.5,\n",
    "        dataset_name: str = \"sample\",\n",
    "    ):\n",
    "        self.image_paths = sorted(glob(os.path.join(data_dir, \"*.png\")))\n",
    "        self.mask_paths = sorted(glob(os.path.join(data_dir, \"*.npy\")))\n",
    "        self.train = train\n",
    "        self.mask_size = mask_size\n",
    "        self.num_parts = num_parts\n",
    "        self.min_crop_ratio = min_crop_ratio\n",
    "        self.current_part_idx = 1\n",
    "\n",
    "        \n",
    "        # Normalize transform (similar to CLIP normalization)\n",
    "        self.normalize_transform = A.Compose([\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "        # For rotation angle range\n",
    "        if dataset_name == \"celeba\":\n",
    "            rotation_range = (-10, 10)\n",
    "        else:\n",
    "            rotation_range = (-30, 30)\n",
    "            \n",
    "        # Train transforms\n",
    "        self.train_transform_1 = A.Compose([\n",
    "            A.Resize(352, 352),\n",
    "            A.HorizontalFlip(),\n",
    "            A.GaussianBlur(blur_limit=(3, 5))\n",
    "        ])\n",
    "        \n",
    "        self.train_transform_2 = A.Compose([\n",
    "            A.Rotate(\n",
    "                rotation_range,\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "                value=0,\n",
    "                mask_value=0\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Test transform\n",
    "        self.tensorize = A.Compose([\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "        self.resize_transform = A.Compose([\n",
    "            A.Resize(352,352)\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        image = np.array(Image.open(self.image_paths[idx]))\n",
    "        name = self.image_paths[idx].split('/')[-1]\n",
    "        name = name.replace('.png', '')\n",
    "        \n",
    "        if self.train:\n",
    "            # Load mask\n",
    "            mask = np.load(self.mask_paths[idx])\n",
    "            \n",
    "            # Apply first set of transforms (resize, flip, blur)\n",
    "            result = self.train_transform_1(image=image, mask=mask)\n",
    "            image, mask = result[\"image\"], result[\"mask\"]\n",
    "                        \n",
    "            \n",
    "            # Apply second set of transforms (rotation)\n",
    "            result = self.train_transform_2(image=image, mask=mask)\n",
    "            image, mask = result[\"image\"], result[\"mask\"]\n",
    "            \n",
    "            # Convert to tensor and normalize\n",
    "            result = self.normalize_transform(image=image)\n",
    "            result = self.tensorize(image = result['image'])\n",
    "            image = result[\"image\"]\n",
    "            \n",
    "            # Convert mask to tensor\n",
    "            mask = torch.from_numpy(mask).float()\n",
    "            \n",
    "            return image, mask, name\n",
    "        else:\n",
    "            # Test mode\n",
    "            mask = np.load(self.mask_paths[idx])\n",
    "            result = self.resize_transform(image = image, mask = mask)\n",
    "            image, mask = result[\"image\"], result[\"mask\"]\n",
    "            result = self.normalize_transform(image=image)\n",
    "            image = result['image']\n",
    "            # should I turn to tensor?\n",
    "            result = self.tensorize(image=image)\n",
    "            image = result[\"image\"]\n",
    "            mask = torch.from_numpy(mask).float()\n",
    "                \n",
    "            return image, mask, name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data_dir: str = \"./data\",\n",
    "        val_data_dir: str = \"./data\",\n",
    "        test_data_dir: str = \"./data\",\n",
    "        batch_size: int = 1,\n",
    "        train_mask_size: int = 352,\n",
    "        test_mask_size: int = 352,\n",
    "        num_parts: int = 2,\n",
    "        min_crop_ratio: float = 0.5,\n",
    "        dataset_name: str = \"sample\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_data_dir = train_data_dir\n",
    "        self.val_data_dir = val_data_dir\n",
    "        self.test_data_dir = test_data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.train_mask_size = train_mask_size\n",
    "        self.test_mask_size = test_mask_size\n",
    "        self.num_parts = num_parts\n",
    "        self.min_crop_ratio = min_crop_ratio\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = Dataset(\n",
    "                data_dir=self.train_data_dir,\n",
    "                train=True,\n",
    "                mask_size=self.train_mask_size,\n",
    "                num_parts=self.num_parts,\n",
    "                min_crop_ratio=self.min_crop_ratio,\n",
    "                dataset_name=self.dataset_name,\n",
    "            )\n",
    "            self.val_dataset = Dataset(\n",
    "                data_dir=self.val_data_dir,\n",
    "                train=False,\n",
    "                mask_size=self.test_mask_size,\n",
    "            )\n",
    "        elif stage == \"test\":\n",
    "            self.test_dataset = Dataset(\n",
    "                data_dir=self.test_data_dir,\n",
    "                train=False,\n",
    "                mask_size=self.test_mask_size,\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, batch_size=self.batch_size, num_workers=0, shuffle=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset, batch_size=self.batch_size, num_workers=0, shuffle=False\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset, batch_size=self.batch_size, num_workers=0, shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        # Ensure shapes match\n",
    "        if predictions.shape != targets.shape:\n",
    "            targets = targets.view(predictions.shape)\n",
    "            \n",
    "        # Flatten prediction and target tensors\n",
    "        predictions = predictions.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (predictions * targets).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (\n",
    "            predictions.sum() + targets.sum() + self.smooth\n",
    "        )\n",
    "        \n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from kornia.contrib import distance_transform\n",
    "\n",
    "class BoundaryLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BoundaryLoss, self).__init__()\n",
    "\n",
    "    def compute_target_sdf(self, target_mask):\n",
    "        # Ensure target mask is properly shaped for distance_transform\n",
    "        # If target_mask is (B, H, W), we need to add a channel dimension\n",
    "        if target_mask.dim() == 3:\n",
    "            binary_mask = (target_mask > 0).float().unsqueeze(1)  # (B, 1, H, W)\n",
    "        else:\n",
    "            binary_mask = (target_mask > 0).float()\n",
    "\n",
    "        # Compute distance transforms for foreground and background\n",
    "        pos_dist = distance_transform(binary_mask)\n",
    "        neg_dist = distance_transform(1 - binary_mask)\n",
    "\n",
    "        # Calculate signed distance function\n",
    "        sdf = neg_dist - pos_dist  # Note: switched order for correct boundary emphasis\n",
    "        return sdf\n",
    "\n",
    "    def forward(self, pred_mask, target_mask):\n",
    "        \"\"\"\n",
    "        pred_mask: Predicted probabilities after sigmoid (batch_size, 1, height, width)\n",
    "        target_mask: Ground truth binary mask (batch_size, height, width)\n",
    "        \"\"\"\n",
    "        batch_size = pred_mask.size(0)\n",
    "        \n",
    "        # Skip softmax since input is already probability after sigmoid\n",
    "        pred_probs = pred_mask  # Already probabilities, no need for softmax\n",
    "        \n",
    "        # Ensure target_mask is on the correct device\n",
    "        target_mask = target_mask.to(pred_mask.device)\n",
    "        \n",
    "        # Compute SDF for the target mask\n",
    "        target_sdf = self.compute_target_sdf(target_mask)\n",
    "        \n",
    "        # Normalize SDF\n",
    "        target_sdf = torch.tanh(target_sdf / 10.0)\n",
    "        \n",
    "        # For binary case with pre-applied sigmoid, use probabilities directly\n",
    "        weighted_probs = pred_probs.squeeze(1) * torch.abs(target_sdf).squeeze(1)\n",
    "        loss = weighted_probs.sum() / (weighted_probs.numel() / batch_size)\n",
    "        \n",
    "        # Adjust regularization term for binary case\n",
    "        reg_term = 0.1 * torch.mean((pred_probs.squeeze(1) - target_mask.float()) ** 2)\n",
    "        \n",
    "        return loss + reg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, prediction, target):\n",
    "        # Apply sigmoid if needed\n",
    "        prediction = torch.sigmoid(prediction)\n",
    "        \n",
    "        # Flatten the tensors\n",
    "        prediction = prediction.view(-1)\n",
    "        target = target.view(-1).float()\n",
    "        \n",
    "        # Calculate BCE\n",
    "        bce = F.binary_cross_entropy(prediction, target, reduction='none')\n",
    "        \n",
    "        # Calculate focal weight\n",
    "        pt = target * prediction + (1 - target) * (1 - prediction)\n",
    "        focal_weight = (1 - pt) ** self.gamma\n",
    "        \n",
    "        # Calculate focal loss\n",
    "        focal_loss = self.alpha * focal_weight * bce\n",
    "        \n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.5, dice_weight=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = DiceLoss()\n",
    "        \n",
    "    def forward(self, prediction, target):\n",
    "        target = target.float()\n",
    "        if target.dim() == 3:\n",
    "            target = target.unsqueeze(1)  # Add channel dimension\n",
    "            \n",
    "        bce_loss = self.bce(prediction, target)\n",
    "        dice_loss = self.dice(torch.sigmoid(prediction), target)\n",
    "        \n",
    "        return self.bce_weight * bce_loss + self.dice_weight * dice_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# few shot clipseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import gc\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# load the model\n",
    "\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "from models.clipseg import CLIPDensePredT\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CLIPSeg22(pl.LightningModule):\n",
    "    def __init__(self, config, learning_rate=0.001, is_refined = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        # self.save_hyperparameters(config.__dict__)\n",
    "        self.max_val_iou = 0\n",
    "        self.val_ious = []\n",
    "\n",
    "        # self.device1 = 'cpu'\n",
    "        self.device1 = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(self.device1)\n",
    "\n",
    "        # load model\n",
    "        if is_refined:\n",
    "            self.model = CLIPDensePredT(version='ViT-B/16', reduce_dim=64, complex_trans_conv=True)\n",
    "        else: \n",
    "            self.model = CLIPDensePredT(version='ViT-B/16', reduce_dim=64)\n",
    "        self.model.train()\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # non-strict, because we only stored decoder weights (not CLIP weights)\n",
    "        if is_refined:\n",
    "            self.model.load_state_dict(torch.load('clipseg_weights/rd64-uni-refined.pth', map_location=torch.device('cpu')), strict=False)\n",
    "        else:\n",
    "            self.model.load_state_dict(torch.load('clipseg_weights/rd64-uni.pth', map_location=torch.device('cpu')), strict=False)\n",
    "\n",
    "        self.model = self.model.to(self.device1)\n",
    "\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        base_prompt = 'skin cancer'\n",
    "\n",
    "        import clip\n",
    "\n",
    "        # text_tokens = clip.tokenize(base_prompt).to(self.device1)\n",
    "        # emb = self.model.clip_model.encode_text(text_tokens)\n",
    "        # self.emb_to_learn = emb.detach().clone().to(self.device1)  # Detach and clone to make it a leaf tensor\n",
    "        # self.emb_to_learn.requires_grad_(True)  # Correct way to set requires_grad\n",
    "\n",
    "        # Create embedding properly as a model parameter\n",
    "        with torch.no_grad():\n",
    "            text_tokens = clip.tokenize(base_prompt).to(self.device1)\n",
    "            emb = self.model.clip_model.encode_text(text_tokens)\n",
    "            self.emb_to_learn = torch.nn.Parameter(emb.clone())\n",
    "\n",
    "        self.i = 0\n",
    "\n",
    "\n",
    "\n",
    "    def on_fit_start(self) -> None: #pl: called at the beginning of the fit()\n",
    "        # move model to gpu\n",
    "        self.start_time = time.time()\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        image, mask, _ = batch\n",
    "        image = image.to(self.device1)\n",
    "        mask = mask.to(self.device1)\n",
    "                \n",
    "        # Get model predictions\n",
    "\n",
    "        preds = self.model(image, self.emb_to_learn)[0]\n",
    "\n",
    "        \n",
    "        # Apply sigmoid but keep the computational graph intact\n",
    "        prediction = torch.sigmoid(preds)\n",
    "        \n",
    "        # Make sure mask and prediction have compatible shapes\n",
    "        # print('pred shape',prediction.shape) # pred shape torch.Size([1, 1, 352, 352])\n",
    "        # print('gt shape', mask.shape)# gt shape torch.Size([1, 352, 352])\n",
    "\n",
    "        # # # Use Boundary loss\n",
    "        # bloss = BoundaryLoss().to(self.device1)\n",
    "        # loss = bloss(prediction, mask)\n",
    "\n",
    "\n",
    "        \n",
    "        # Use DiceLoss\n",
    "        # Assuming mask is [B, H, W] and prediction is [B, 1, H, W]\n",
    "        if len(mask.shape) < len(prediction.shape):\n",
    "            mask = mask.unsqueeze(1)\n",
    "        dice = DiceLoss().to(self.device1)\n",
    "        loss = dice(prediction, mask)\n",
    "\n",
    "        # loss = loss1 + loss2\n",
    "\n",
    "\n",
    "        # criterion = nn.BCELoss()\n",
    "        # loss = criterion(prediction, mask.unsqueeze(1).float())\n",
    "\n",
    "        ##combined loss\n",
    "        # combined_loss = CombinedLoss()\n",
    "        # loss = combined_loss(prediction, mask)\n",
    "        self.log(\"loss\", loss.detach().cpu(), on_epoch=True)\n",
    "        \n",
    "        \n",
    "        if self.i % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "        \n",
    "        self.i = self.i + 1\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def on_validation_start(self):\n",
    "        pass\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        os.makedirs('validations', exist_ok=True)\n",
    "        self.val_ious = []\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        #calculate mask / iou\n",
    "        image, mask, file_name = batch\n",
    "        file_name = file_name[0]\n",
    "        image = image.to(self.device1)\n",
    "        mask = mask.to(self.device1)\n",
    "\n",
    "\n",
    "\n",
    "        # predict\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(image, self.emb_to_learn)[0]\n",
    "            # Apply sigmoid but keep the computational graph intact\n",
    "            preds = torch.sigmoid(preds)\n",
    "\n",
    "        #mask generation\n",
    "        # filename = f\"mask.png\"\n",
    "        # here we save the second mask\n",
    "        m = torch.sigmoid(preds[0][0]).cpu()\n",
    "        plt.imsave(f'validations/{file_name}.png',m)\n",
    "\n",
    "\n",
    "        # Create binary mask\n",
    "        img2 = cv2.imread(f'validations/{file_name}.png')\n",
    "        gray_image = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "        (thresh, bw_image) = cv2.threshold(gray_image, 125, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Save the binary mask using cv2.imwrite instead of plt.imsave\n",
    "        bmask_filename = f\"validations/{file_name}_bmask.png\"\n",
    "        cv2.imwrite(bmask_filename, bw_image)\n",
    "\n",
    "        iou = calculate_iou(bw_image, mask[0].cpu().numpy())\n",
    "\n",
    "        self.val_ious.append(iou)\n",
    "\n",
    "        # self.log(\"val mean iou\", mean_iou.cpu(), on_step=True, sync_dist=True)\n",
    "        return torch.tensor(0.0)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        mean_iou = sum(self.val_ious) / len(self.val_ious)\n",
    "        print('mean iou is ', mean_iou)\n",
    "        self.log(\"val mean iou\", mean_iou ,on_epoch=True)\n",
    "        gc.collect() #python garbage collector\n",
    "\n",
    "    def on_test_start(self) -> None:\n",
    "        # make directory for saving results\n",
    "        self.end_time = time.time()\n",
    "\n",
    "        print(f\"Time taken for training : {self.end_time - self.start_time} seconds\")\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        # load the embeddings if needed\n",
    "        self.start_time = time.time()\n",
    "        pass\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Saving just the emb_to_learn parameter\n",
    "        torch.save(self.emb_to_learn, 'emb_to_learn.pt')\n",
    "\n",
    "\n",
    "        image, mask, file_name = batch\n",
    "        file_name = file_name[0]\n",
    "        image = image.to(self.device1)\n",
    "        mask = mask.to(self.device1)\n",
    "\n",
    "\n",
    "\n",
    "        # predict\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(image, self.emb_to_learn)[0]\n",
    "            # Apply sigmoid but keep the computational graph intact\n",
    "            preds = torch.sigmoid(preds)\n",
    "\n",
    "        #mask generation\n",
    "        # filename = f\"mask.png\"\n",
    "        # here we save the second mask\n",
    "        m = torch.sigmoid(preds[0][0]).cpu()\n",
    "        plt.imsave(f'results/{file_name}.png',m)\n",
    "\n",
    "\n",
    "        # Create binary mask\n",
    "        img2 = cv2.imread(f'results/{file_name}.png')\n",
    "        gray_image = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "        (thresh, bw_image) = cv2.threshold(gray_image, 125, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Save the binary mask using cv2.imwrite instead of plt.imsave\n",
    "        bmask_filename = f\"results/{file_name}_bmask.png\"\n",
    "        cv2.imwrite(bmask_filename, bw_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return torch.tensor(0.0)\n",
    "\n",
    "    def on_test_end(self) -> None:\n",
    "        self.end_time = time.time()\n",
    "\n",
    "        print(f\"Time taken for {len(self.trainer.datamodule.test_dataset)} images: {self.end_time - self.start_time} seconds\")\n",
    "        print(f\"Average time per image: {(self.end_time - self.start_time) / len(self.trainer.datamodule.test_dataset)} seconds\")\n",
    "        print(\"max val mean iou: \", self.max_val_iou)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = getattr(optim, self.config.optimizer)(\n",
    "            [self.emb_to_learn],  # Pass the parameter directly\n",
    "            lr=self.lr,\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def on_before_optimizer_step(self, optimizer):\n",
    "        pass\n",
    "        # if self.trainer.global_step % 1 == 0:  # Avoid excessive logging\n",
    "        #   if self.emb_to_learn.grad is not None:\n",
    "        #       print(\"Embedding grad in training step:\",\n",
    "        #           self.emb_to_learn.grad.min().item(),\n",
    "        #           self.emb_to_learn.grad.mean().item(),\n",
    "        #           self.emb_to_learn.grad.max().item())\n",
    "        #   else:\n",
    "        #       print(\"Embedding grad is None in training step\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b887a23670c415298084083dc73106e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean iou is  101.91423\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e764ddc79add4ecb8c5f69f36c9edc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean iou is  104.01138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee035fa3e694a5a9406be7e566d7531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training : 99.17919158935547 seconds\n",
      "Time taken for 191 images: 9.219825029373169 seconds\n",
      "Average time per image: 0.04827133523232026 seconds\n",
      "max val mean iou:  0\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "def main():\n",
    "    config = Arguments()\n",
    "    config.dataset_name = \"pascal\"\n",
    "    config.train_data_dir = \"datasets/dataset/train\"\n",
    "    config.val_data_dir = \"datasets/dataset/val\"\n",
    "    config.test_data_dir = \"datasets/dataset/test\"\n",
    "    config.train = True\n",
    "    config.epochs = 200\n",
    "\n",
    "\n",
    "    dm = DataModule(\n",
    "        train_data_dir=config.train_data_dir,\n",
    "        val_data_dir=config.val_data_dir,\n",
    "        test_data_dir=config.test_data_dir,\n",
    "        batch_size=config.batch_size,\n",
    "        test_mask_size=config.test_mask_size,\n",
    "        dataset_name=config.dataset_name,\n",
    "    )\n",
    "    model = CLIPSeg22(config=config, is_refined=True)\n",
    "    if isinstance(config.gpu_id, int):\n",
    "        gpu_id = [config.gpu_id]\n",
    "    else:\n",
    "        gpu_id = config.gpu_id\n",
    "    trainer = pl.Trainer(\n",
    "        # accelerator=\"gpu\",\n",
    "        default_root_dir=config.output_dir,\n",
    "        max_epochs=config.epochs,\n",
    "        devices=gpu_id,\n",
    "        enable_checkpointing=False,\n",
    "        num_sanity_val_steps=0,\n",
    "        val_check_interval= 1.0,\n",
    "    )\n",
    "    if config.train:\n",
    "        trainer.fit(model=model, datamodule=dm)\n",
    "        trainer.test(model=model, datamodule=dm)\n",
    "    else:\n",
    "        trainer.test(model=model, datamodule=dm)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Later, to load it:\n",
    "# loaded_emb = torch.load('emb_to_learn.pt')\n",
    "# self.emb_to_learn = torch.nn.Parameter(loaded_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
